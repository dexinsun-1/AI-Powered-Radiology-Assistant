{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":["3v5RPp87N6i-","LnW3ItT5_8iI","6EZVCcPFAHCS","r2SpRVGLAeLN","8U2-2DliHo1n","S5aBWYc3K9iV"],"authorship_tag":"ABX9TyN6ub5rSkZni+JLSqkHK9Ta"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a873c25bfe874046ab5cd682b669a289":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ef9beec5acf4e3da8b022b935baed5b","IPY_MODEL_4a8f0df2e8be492fb23f90e27d2fd0d8","IPY_MODEL_58af047f2766405a92e3bfa5ac7f19fe"],"layout":"IPY_MODEL_2eee138731164839a08ba49b7fa2fb99"}},"7ef9beec5acf4e3da8b022b935baed5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d8ae433e7c147fd87bd1c2b5964e1e6","placeholder":"​","style":"IPY_MODEL_efa65244d80941569a9a9c1a2e406627","value":"Loading checkpoint shards: 100%"}},"4a8f0df2e8be492fb23f90e27d2fd0d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72e315a3aa5d47579dccabb26b7f5ba7","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9136e09f02084f02919cf366740acc7b","value":2}},"58af047f2766405a92e3bfa5ac7f19fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33cc4b5e6e4f477983e2d12b24600592","placeholder":"​","style":"IPY_MODEL_a2980e94d836499c8ec832b8628f8bef","value":" 2/2 [00:04&lt;00:00,  2.08s/it]"}},"2eee138731164839a08ba49b7fa2fb99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d8ae433e7c147fd87bd1c2b5964e1e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efa65244d80941569a9a9c1a2e406627":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72e315a3aa5d47579dccabb26b7f5ba7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9136e09f02084f02919cf366740acc7b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33cc4b5e6e4f477983e2d12b24600592":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2980e94d836499c8ec832b8628f8bef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# ***Prepare***"],"metadata":{"id":"3v5RPp87N6i-"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/python_env/lib/python3.10/site-packages')\n","\n","project_path = \"/content/drive/MyDrive/AI_MRI_Project\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pWfAtmikMGE2","executionInfo":{"status":"ok","timestamp":1741514511117,"user_tz":240,"elapsed":646,"user":{"displayName":"Gao Wayne","userId":"08103769368950608599"}},"outputId":"95af6465-324d-4ef4-990d-97e119f7d8d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D2Q-QxWJBnjS","executionInfo":{"status":"ok","timestamp":1741514529000,"user_tz":240,"elapsed":9746,"user":{"displayName":"Gao Wayne","userId":"08103769368950608599"}},"outputId":"267a79da-8461-47bf-db2f-0fbc276a92ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: write).\n","The token `EECS6895-Ass1` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `EECS6895-Ass1`\n"]}]},{"cell_type":"markdown","source":["# ***Load Model***"],"metadata":{"id":"LnW3ItT5_8iI"}},{"cell_type":"code","source":["# Import necessary libraries\n","import torch\n","from transformers import BitsAndBytesConfig\n","from peft import PeftModel\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","\n","# ======================= CONFIGURATION =======================\n","# The original configuration for BitsAndBytesConfig was commented out.\n","# BitsAndBytesConfig is useful for loading models with 4-bit quantization for reduced memory consumption.\n","# This configuration is optimal for balancing performance and precision when using large models like LLaMA.\n","#\n","# Example configuration for reference:\n","# bnb_config = BitsAndBytesConfig(\n","#     load_in_4bit=True,               # ✅ Load the model in 4-bit precision to reduce memory consumption\n","#     bnb_4bit_compute_dtype=torch.float16, # ✅ Use FP16 for efficient computation with minimal precision loss\n","#     bnb_4bit_use_double_quant=True,  # ✅ Apply second-layer quantization to improve model performance\n","#     bnb_4bit_quant_type=\"nf4\",       # ✅ Use NF4 (Normalized Float 4) for better accuracy in 4-bit models\n","# )\n","\n","# ======================= BASE MODEL LOADING =======================\n","# Define the base model name using Meta's LLaMA 2 7B chat variant.\n","base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n","\n","# Load the base LLaMA model for causal language modeling (causal LM).\n","# Causal LM predicts the next token based on the previous tokens.\n","base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n","\n","# Load the tokenizer for the base model.\n","# Tokenizer is responsible for text tokenization and detokenization processes.\n","base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","\n","# ======================= LoRA FINE-TUNED MODEL LOADING =======================\n","# Path to the fine-tuned model trained with PEFT (Parameter Efficient Fine-Tuning)\n","fine_tuned_model_path = '/content/drive/MyDrive/AI_MRI_Project/fine_tuned_models'\n","\n","# Load the fine-tuned model using PEFT, which efficiently updates fewer model parameters for faster training and improved memory efficiency.\n","fine_tuned_model = PeftModel.from_pretrained(base_model, fine_tuned_model_path)\n","\n","# Use the same tokenizer for the fine-tuned model to ensure consistency in text encoding/decoding.\n","fine_tuned_tokenizer = base_tokenizer\n","\n","# ======================= TEXT GENERATION PIPELINES =======================\n","# Pipeline for text generation using the base LLaMA model\n","# `torch_dtype=torch.float16` ensures reduced memory usage and faster computations using FP16 precision.\n","# `device_map=\"auto\"` automatically maps the model to available GPUs or CPUs for optimized performance.\n","\n","# Pipeline for text generation using the fine-tuned LLaMA model\n","# Fine-tuned models often demonstrate improved accuracy for domain-specific tasks.\n","fine_tuned_generator = pipeline(\n","    \"text-generation\",  # Defines the task as text generation\n","    model=fine_tuned_model,  # Fine-tuned PEFT model\n","    tokenizer=fine_tuned_tokenizer,  # Consistent tokenizer for the fine-tuned model\n","    torch_dtype=torch.float16,  # FP16 for better speed and memory efficiency\n","    device_map=\"auto\"  # Automatically allocate computing resources for efficient model inference\n",")\n","\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225,"referenced_widgets":["a873c25bfe874046ab5cd682b669a289","7ef9beec5acf4e3da8b022b935baed5b","4a8f0df2e8be492fb23f90e27d2fd0d8","58af047f2766405a92e3bfa5ac7f19fe","2eee138731164839a08ba49b7fa2fb99","6d8ae433e7c147fd87bd1c2b5964e1e6","efa65244d80941569a9a9c1a2e406627","72e315a3aa5d47579dccabb26b7f5ba7","9136e09f02084f02919cf366740acc7b","33cc4b5e6e4f477983e2d12b24600592","a2980e94d836499c8ec832b8628f8bef"]},"id":"ZJiMSPmXPoAJ","executionInfo":{"status":"ok","timestamp":1741514562862,"user_tz":240,"elapsed":14434,"user":{"displayName":"Gao Wayne","userId":"08103769368950608599"}},"outputId":"ea358521-6524-4002-f1cc-5d8e64b62d87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a873c25bfe874046ab5cd682b669a289"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n","Device set to use cuda:0\n","The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n"]}]},{"cell_type":"markdown","source":["# ***Findings***"],"metadata":{"id":"6EZVCcPFAHCS"}},{"cell_type":"code","source":["import re\n","\n","def clean_mri_findings_output(raw_output):\n","    \"\"\"\n","    Cleans LLaMA model output and extracts relevant MRI Findings information.\n","\n","    This function is designed to process raw output generated by the LLaMA model,\n","    identifying and extracting content specifically labeled as \"MRI Findings.\"\n","    It relies on a structured format that includes '**BEGIN OUTPUT**' as a key marker.\n","\n","    Parameters:\n","    - raw_output (str): The raw text output generated by the LLaMA model.\n","\n","    Returns:\n","    - str: The extracted MRI Findings content.\n","           If no valid content is detected, an error message is returned.\n","    \"\"\"\n","\n","    # ======================= Regular Expression Pattern =======================\n","    # The regex pattern is designed to:\n","    # 🔹 Locate content starting immediately after '**BEGIN OUTPUT**' (ignoring surrounding text)\n","    # 🔹 Capture all content until the next double newline `\\n\\n` or the end of the text `$`.\n","    # 🔹 `(?<=...)` is a **positive lookbehind assertion** — it ensures the match starts\n","    #    immediately after the '**BEGIN OUTPUT**' marker but excludes the marker itself.\n","    # 🔹 `(?=\\n\\n|$)` is a **positive lookahead assertion** — it stops the match before\n","    #    encountering either a double newline or the end of the text.\n","\n","    pattern = r\"(?<=\\*\\*BEGIN OUTPUT\\*\\*\\n)(.*?)(?=\\n\\n|$)\"\n","\n","    # ======================= Perform Regex Search =======================\n","    # `re.search()` is used because:\n","    # 🔹 It efficiently finds the first match within the text.\n","    # 🔹 `re.DOTALL` is used to ensure `.` matches newline characters,\n","    #    allowing multi-line MRI findings content to be captured.\n","    match = re.search(pattern, raw_output, re.DOTALL)\n","\n","    # ======================= Extraction & Return =======================\n","    # ✅ If a match is found, return the cleaned content (trimmed for clarity)\n","    if match:\n","        return match.group(0).strip()\n","\n","    # ❌ If no match is found, return an error message\n","    # This message is useful for debugging prompts, model outputs, or formatting issues.\n","    else:\n","        return \"❌ MRI Findings not found. Please check the prompt format or model output.\"\n","\n"],"metadata":{"id":"ZEF1CE4oSQds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_mri_findings(mri_description, llama_model):\n","    \"\"\"\n","    Generates MRI Findings based on a provided MRI imaging description using LLaMA model.\n","\n","    This function employs a One-Shot Prompting technique, where a structured example is given\n","    to guide the model toward generating accurate and standardized MRI findings.\n","\n","    Parameters:\n","    - mri_description (str): The descriptive text explaining MRI imaging observations.\n","    - llama_model (Pipeline): The LLaMA model (or other compatible model) for text generation.\n","\n","    Returns:\n","    - str: The generated MRI Findings output based on the given description.\n","    \"\"\"\n","\n","    # ======================= Step 1: Prompt Engineering =======================\n","    # ✅ Prompt Design Strategy: One-Shot Prompting\n","    # One-shot prompting provides the model with:\n","    #   1. Clear task instructions\n","    #   2. An example output in the desired format\n","    #   3. The actual MRI imaging description for the model to interpret\n","    # This structured format helps the model produce coherent and standardized text output.\n","\n","    prompt = f\"\"\"\n","    Generate the 'Findings' section of a standard MRI report based on the following MRI imaging description.\n","\n","    **Imaging Description**:\n","    MRI scan shows a well-defined extra-axial mass in the left parietal region, measuring 3.0 cm.\n","    The lesion is isointense on T1-weighted imaging and hyperintense on T2-weighted imaging.\n","    The mass exhibits a clear boundary with localized growth extending along the dura.\n","    There is no evidence of surrounding edema or significant mass effect.\n","\n","    **Example Output (Follow this format)**:\n","    - **Tumor Classification**: Meningioma\n","    - **Location**: Left parietal region\n","    - **Size/Extent**: 3.0 cm\n","    - **Type**: Isointense on T1-weighted imaging, hyperintense on T2-weighted imaging\n","    - **Distribution Pattern**: Extra-axial lesion with clear boundaries, localized growth pattern\n","    - **Mass Effect**: No significant mass effect or surrounding edema\n","\n","    Now, based on the following MRI imaging description, generate the corresponding 'Findings' section:\n","\n","    **Imaging Description**:\n","    {mri_description}\n","\n","    **BEGIN OUTPUT**\n","    \"\"\"\n","\n","    # ======================= Step 2: Model Generation =======================\n","    # ✅ LLaMA Model Inference\n","    # `max_length=2048` is set to accommodate detailed MRI findings descriptions.\n","    # - Large medical descriptions and multi-feature outputs may require extended token limits.\n","    # `temperature=0.3` is selected to:\n","    # - Reduce randomness in the output for more precise and factual text generation.\n","    # - Lower temperature values encourage more deterministic outputs, which are ideal for medical content.\n","    response = llama_model(prompt, max_length=2048, temperature=0.3)[0][\"generated_text\"]\n","\n","    # ✅ Return the generated MRI findings text\n","    return response\n","\n"],"metadata":{"id":"tlt1cGMnJ_uS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mri_description = \"MRI scan reveals a large hyperintense mass in the right frontal lobe, measuring approximately 4.5 cm. The lesion demonstrates irregular margins with heterogeneous enhancement following contrast administration. Significant vasogenic edema is present, causing a midline shift of approximately 5 mm. There is compression of the right lateral ventricle with partial effacement of the sulci.\"\n"],"metadata":{"id":"5ezTN3ANHvzw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mri_description = \"Symmetrical enhancement in the brainstem, particularly within the pons region, indicating potential inflammatory, neoplastic, or vascular pathology. A hyperintense lesion with irregular enhancement in the posterior fossa, notably in the cerebellum, suggesting possible secondary involvement or extension.No significant midline shift, ventricular compression, or sulcal effacement observed. The surrounding brain parenchyma appears structurally preserved, with no signs of acute hemorrhage or ischemia.\""],"metadata":{"id":"-jn4ICx3svBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate MRI Findings using LLaMA\n","raw_mri_findings = generate_mri_findings(mri_description, fine_tuned_generator)\n","\n","# Clean the model's output to extract MRI findings only\n","mri_findings = clean_mri_findings_output(raw_mri_findings)\n","\n","# Display Results\n","print(\"📑 MRI Findings:\\n\", mri_findings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yp1QBuBMSXpV","executionInfo":{"status":"ok","timestamp":1741436557288,"user_tz":300,"elapsed":6,"user":{"displayName":"Gao Wayne","userId":"08103769368950608599"}},"outputId":"168f0d03-bdef-4b47-d1f7-2af3aabbbc2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["📑 MRI Findings:\n"," **Tumor Classification**: None identified\n","    **Location**: Brainstem and posterior fossa\n","    **Size/Extent**: N/A\n","    **Type**: Symmetrical enhancement in the brainstem, hyperintense lesion in the posterior fossa\n","    **Distribution Pattern**: Brainstem and posterior fossa involvement with irregular enhancement\n","    **Mass Effect**: No significant mass effect or surrounding edema\n"]}]},{"cell_type":"markdown","source":["# ***Risk Assessment***"],"metadata":{"id":"r2SpRVGLAeLN"}},{"cell_type":"code","source":["def assess_mri_risk(mri_findings, llama_model):\n","    \"\"\"\n","    Assess the MRI risk level based on MRI findings using the LLaMA model.\n","\n","    This function utilizes a structured prompt that guides the model to:\n","    - Follow a predefined scoring system for risk assessment.\n","    - Provide both a numerical score and a detailed justification.\n","    - Clearly explain how each MRI characteristic contributes to the final risk score.\n","\n","    Parameters:\n","    - mri_findings (str): The detailed MRI findings containing tumor classification, size, location, etc.\n","    - llama_model (Pipeline): The LLaMA model (or any text generation model) used for inference.\n","\n","    Returns:\n","    - str: The generated risk assessment section with detailed scoring and justifications.\n","    \"\"\"\n","\n","    # ======================= Step 1: Prompt Engineering =======================\n","\n","    # ✅ Prompt Design Strategy: Structured Prompting\n","    # The prompt uses a clear structure with:\n","    # 1️⃣ Example Output: Demonstrates the expected structure and format.\n","    # 2️⃣ Scoring Guidelines: Guides the model on how to assign risk scores based on MRI characteristics.\n","    # 3️⃣ MRI Findings: Injects the provided MRI findings for customized risk assessment.\n","\n","    prompt = f\"\"\"\n","    Generate the 'Risk Assessment' section of a standard MRI report based on the following MRI findings.\n","\n","    **Example Output (Follow this format)**:\n","    - **Risk Score**: [Total score out of 100]\n","    - **Risk Level**: [Low/Medium/High]\n","    - **Justification**:\n","      - **Tumor Classification**: [Explain contribution to risk score]\n","      - **Location**: [Explain contribution to risk score]\n","      - **Size/Extent**: [Explain contribution to risk score]\n","      - **Distribution Pattern**: [Explain contribution to risk score]\n","      - **Mass Effect**: [Explain contribution to risk score]\n","\n","    # ======================= Step 2: Scoring Guidelines =======================\n","    # ✅ Clear and Weighted Scoring Rules\n","    # Each MRI characteristic is assigned a percentage weight (e.g., Tumor Classification = 20%).\n","    # The model uses these guidelines to calculate a final risk score (out of 100).\n","\n","    **Scoring Guidelines** (Use this to calculate the score):\n","\n","    1️⃣ **Tumor Classification (20%)**:\n","      - 🔴 **High Risk (Score 18-20)**: Aggressive tumors like Glioblastoma (GBM) or high-grade gliomas\n","      - 🟡 **Medium Risk (Score 8-17)**: Intermediate-grade tumors like anaplastic astrocytoma\n","      - 🟢 **Low Risk (Score 0-7)**: Low-grade gliomas, benign tumors\n","\n","    2️⃣ **Location (15%)**:\n","      - 🔴 **High Risk (Score 13-15)**: Located in critical regions (brainstem, thalamus, basal ganglia)\n","      - 🟡 **Medium Risk (Score 6-12)**: Near functional cortex but non-invasive\n","      - 🟢 **Low Risk (Score 0-5)**: Located in non-critical peripheral regions\n","\n","    3️⃣ **Size/Extent (30%)**:\n","      - 🔴 **High Risk (Score 28-30)**: > 4 cm\n","      - 🟡 **Medium Risk (Score 15-27)**: 2 - 4 cm\n","      - 🟢 **Low Risk (Score 0-14)**: < 2 cm\n","\n","    4️⃣ **Distribution Pattern (15%)**:\n","      - 🔴 **High Risk (Score 13-15)**: Irregular margins with signs of tissue infiltration\n","      - 🟡 **Medium Risk (Score 6-12)**: Localized lesion with mild expansion\n","      - 🟢 **Low Risk (Score 0-5)**: Well-defined borders, no tissue infiltration\n","\n","    5️⃣ **Mass Effect (20%)**:\n","      - 🔴 **High Risk (Score 18-20)**: Midline shift ≥ 5 mm, severe vasogenic edema\n","      - 🟡 **Medium Risk (Score 8-17)**: Mild mass effect, localized sulci effacement\n","      - 🟢 **Low Risk (Score 0-7)**: No significant mass effect\n","\n","    # ======================= Step 3: Insert MRI Findings =======================\n","    # ✅ MRI findings dynamically inserted\n","    # The prompt appends the provided `mri_findings` for the model to analyze.\n","\n","    Now, based on the following MRI findings, generate the corresponding 'Risk Assessment' section:\n","\n","    **MRI Findings**:\n","    {mri_findings}\n","\n","    **BEGIN OUTPUT**\n","    \"\"\"\n","\n","    # ======================= Step 4: Model Inference =======================\n","\n","    # ✅ LLaMA Model Inference\n","    # `max_length=2048` ensures sufficient token space for complex reports.\n","    # `temperature=0.3` reduces randomness, promoting clear and factual outputs.\n","    response = llama_model(prompt, max_length=2048, temperature=0.3)[0][\"generated_text\"]\n","\n","    # ======================= Step 5: Return the Response =======================\n","    return response\n","\n"],"metadata":{"id":"5L1I7BeURGwz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","def extract_output_section(raw_text):\n","    \"\"\"\n","    Extracts the content between '**BEGIN OUTPUT**' and '**END OUTPUT**'.\n","\n","    This function uses a regular expression (regex) pattern to locate and extract\n","    the desired text section. The expected format is:\n","\n","    **BEGIN OUTPUT**\n","    [Desired Content]\n","    **END OUTPUT**\n","\n","    Parameters:\n","    - raw_text (str): The raw text containing the desired content.\n","\n","    Returns:\n","    - str: The extracted text content. If no valid content is found,\n","           an appropriate error message is returned.\n","    \"\"\"\n","\n","    # ======================= Step 1: Regular Expression Design =======================\n","    # ✅ Regex Pattern: r\"\\*\\*BEGIN OUTPUT\\*\\*(.*?)\\*\\*END OUTPUT\\*\\*\"\n","    # Explanation:\n","    # 🔹 `\\*\\*` — Escapes the `**` marker (required because `*` is a special character in regex).\n","    # 🔹 `BEGIN OUTPUT` and `END OUTPUT` — Keywords marking the boundaries of the desired text.\n","    # 🔹 `(.*?)` — Captures text **non-greedily** between the markers.\n","    #     - `?` ensures the search stops at the **first** `**END OUTPUT**` it encounters.\n","    # 🔹 `re.DOTALL` — Enables `.` to match newline characters, ensuring multi-line content is captured.\n","\n","    extracted_text = re.search(r\"\\*\\*BEGIN OUTPUT\\*\\*(.*?)\\*\\*END OUTPUT\\*\\*\",\n","                               raw_text,\n","                               re.DOTALL)\n","\n","    # ======================= Step 2: Extract & Return Results =======================\n","    # ✅ Check if a match was found\n","    if extracted_text:\n","        # ✅ `group(1)` extracts the captured content inside the `(.*?)`\n","        # `.strip()` is used to remove leading/trailing spaces for cleaner output.\n","        return extracted_text.group(1).strip()\n","\n","    # ❌ If no valid content is found, return an error message\n","    else:\n","        return \"❗️ Unable to locate content. Please check the original text format.\"\n"],"metadata":{"id":"6Wb2vaJQeAHK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: Generate Raw Risk Assessment\n","# -------------------------------------\n","# ✅ Generates raw MRI risk assessment text using the fine-tuned generator model\n","# `mri_description` should contain the MRI report's detailed findings\n","raw_risk_assessment = assess_mri_risk(mri_description, fine_tuned_generator)\n","\n","# Step 2: Extract Cleaned Risk Assessment\n","# ---------------------------------------\n","# ✅ Extracts only the relevant content from the generated text\n","# The `extract_output_section()` function isolates content between \"**BEGIN OUTPUT**\" and \"**END OUTPUT**\"\n","risk_assessment = extract_output_section(raw_risk_assessment)\n","\n","# Step 3: Display the Final Risk Assessment\n","# -----------------------------------------\n","# ✅ Prints the cleaned MRI risk assessment for clear presentation\n","print('Risk Assessment')\n","print(risk_assessment)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wfL6dmNPd_89","executionInfo":{"status":"ok","timestamp":1741601127992,"user_tz":240,"elapsed":5,"user":{"displayName":"Gao Wayne","userId":"08103769368950608599"}},"outputId":"636e8a58-922c-49d7-e34c-fbc964b2cc7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Risk Assessment\n","   The risk of malignancy is estimated to be [High (70/100)]\n","   The contribution to the risk score is as follows:\n","        - Tumor Classification: Contributes 20% to the total risk score. The glioma classification is consistent with a medium to\n","                                high-grade tumor, which accounts for [80/100] of the total risk score.\n","        - Location: Contributes 15% to the total risk score. The right frontal lobe location is a medium-risk factor, accounting\n","                       for [12/100] of the total risk score.\n","        - Size/Extent: Contributes 30% to the total risk score. The tumor size of 4.5 cm is a medium-risk factor, accounting for\n","                       [45/100] of the total risk score.\n","        - Distribution Pattern: Contributes 15% to the total risk score. The irregular margins and significant midline shift with\n","                       vasogenic edema are high-risk features, accounting for [22/100] of the total risk score.\n",".       - Mass Effect: Contributes 20% to the total risk score. The significant mass effect with midline shift of approximately 5 mm and\n","                       partial effacement of the sulci is a high-risk feature, accounting for [40/100] of the total risk score.\n"]}]},{"cell_type":"markdown","source":["# ***Treatment Plan***"],"metadata":{"id":"8U2-2DliHo1n"}},{"cell_type":"code","source":["def generate_treatment_plan(mri_findings, risk_level, llama_model):\n","    \"\"\"\n","    Generates a personalized treatment plan based on MRI findings and risk assessment using the LLaMA model.\n","\n","    This function employs One-Shot Prompting, providing the model with:\n","    - A clear task description\n","    - An example input with detailed MRI findings and risk assessment\n","    - A well-structured example output to guide the model's response\n","\n","    Parameters:\n","    - mri_findings (str): MRI diagnostic details including tumor type, location, size, etc.\n","    - risk_level (str): Risk assessment summary (e.g., Low, Medium, or High risk).\n","    - llama_model (Pipeline): The LLaMA model (or compatible generator) used for text generation.\n","\n","    Returns:\n","    - str: A personalized treatment recommendation based on the provided MRI data.\n","    \"\"\"\n","\n","    # ======================= Step 1: Prompt Engineering =======================\n","    # ✅ Prompt Design Strategy: One-Shot Prompting\n","    # This structured prompt effectively guides the LLaMA model to generate reliable outputs.\n","    # Key components:\n","    #   1️⃣ Clear Instructions\n","    #   2️⃣ Example Input (Demonstrating expected data structure)\n","    #   3️⃣ Example Output (Demonstrating desired treatment plan format)\n","    #   4️⃣ Dynamic Insertion of MRI Findings and Risk Level for personalized recommendations\n","\n","    prompt = f\"\"\"\n","    Generate a personalized 'Treatment Recommendations' section of an MRI diagnostic report.\n","    The treatment plan should consider:\n","    - MRI Findings\n","    - Risk Assessment\n","    - Predicted Diseases\n","\n","    **Example Input:**\n","    **MRI Findings**:\n","    - **Tumor Classification**: Meningioma\n","    - **Location**: Left parietal region\n","    - **Size/Extent**: 3.0 cm\n","    - **Type**: Isointense on T1-weighted imaging, hyperintense on T2-weighted imaging\n","    - **Distribution Pattern**: Extra-axial lesion with clear boundaries, localized growth pattern\n","    - **Mass Effect**: No significant mass effect or surrounding edema\n","\n","    **Risk Assessment:** Medium Risk (Score: 55/100)\n","\n","    **Predicted Disease(s)**: Meningioma (85% confidence)\n","\n","    **Example Output (Follow this format):**\n","    - **Imaging Recommendations**: Perform contrast-enhanced MRI to assess vascular involvement.\n","    - **Follow-up Schedule**: MRI follow-up in 3 months to monitor lesion stability.\n","    - **Treatment Suggestions**:\n","        - Surgery may be considered if the lesion shows rapid growth.\n","        - Radiation therapy may be indicated for residual tumor or recurrence.\n","        - Regular monitoring for neurological symptoms is advised.\n","\n","    Now, based on the following information, generate the corresponding 'Treatment Recommendations' section:\n","\n","    **MRI Findings**:\n","    {mri_findings}\n","\n","    **Risk Assessment:** {risk_level}\n","\n","    **BEGIN OUTPUT**\n","    \"\"\"\n","\n","    # ======================= Step 2: Model Inference =======================\n","    # ✅ LLaMA Model Inference\n","    # The following parameters ensure the model generates focused, structured, and accurate content:\n","    # 🔹 `max_length=2048` — Ensures sufficient space for complex treatment recommendations.\n","    # 🔹 `temperature=0.3` — Low temperature favors deterministic, factual content over creative text.\n","    #    - Ideal for generating precise medical content rather than open-ended language.\n","\n","    response = llama_model(prompt, max_length=2048, temperature=0.3)[0][\"generated_text\"]\n","\n","    # ======================= Step 3: Return the Response =======================\n","    return response\n","\n"],"metadata":{"id":"sdX748V2gzuT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: Generate Treatment Plan\n","# --------------------------------\n","# ✅ Uses the `generate_treatment_plan()` function to generate a personalized treatment plan.\n","# `mri_findings` contains the MRI diagnostic details.\n","# `risk_assessment` is the evaluated risk score (e.g., Low, Medium, or High Risk).\n","# `fine_tuned_generator` is the model generating the text output.\n","treatment_plan = generate_treatment_plan(mri_findings, risk_assessment, fine_tuned_generator)\n","\n","# Step 2: Extract Cleaned Treatment Plan\n","# --------------------------------------\n","# ✅ The raw treatment plan may include extra content.\n","# The `extract_output_section()` function isolates only the meaningful part (from **BEGIN OUTPUT** to **END OUTPUT**).\n","treatment_plan = extract_output_section(treatment_plan)\n","\n","# Step 3: Display Treatment Plan\n","# ------------------------------\n","# ✅ Print the clean and structured treatment plan.\n","print(treatment_plan)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVk3pUYLKhMM","executionInfo":{"status":"ok","timestamp":1742635418782,"user_tz":240,"elapsed":27,"user":{"displayName":"Gao Wayne","userId":"08103769368950608599"}},"outputId":"33001209-a001-42b4-89ac-da8a3d7475d4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Based on the MRI findings and risk assessment, the following treatment options are considered:\n","\n","- **Surgical Resection**: Surgical resection may be considered for this patient, given the high risk\n","  of malignancy and significant midline shift. The tumor's location in the right frontal lobe and\n","  its irregular margins may make it challenging to completely resect the tumor, but it is still\n","  the primary treatment option.\n","\n","- **Radiation Therapy**: Radiation therapy may be considered as an adjunct to surgical resection\n","  or as a standalone treatment for any remaining tumor after surgery. The patient's age and overall\n","  health status may affect the radiation therapy plan.\n","\n","- **Conservative Management**: Conservative management with close follow-up and serial MRI may be\n","  considered for this patient, given the high risk of malignancy and significant midline shift.\n","  This approach may be more appropriate for patients who are not candidates for surgical resection\n","  or radiation therapy.\n","\n","- **Clinical Trials**: The patient may be eligible for clinical trials of new treatments for glioma,\n","  given the high risk of malignancy and the patient's age and overall health status.\n","\n","**Follow-up Schedule**: MRI follow-up in 1 month to assess tumor stability and potential response to treatment.\n"]}]},{"cell_type":"markdown","source":["# ***Generate Reports***"],"metadata":{"id":"S5aBWYc3K9iV"}},{"cell_type":"code","source":["def generate_mri_diagnostic_report(mri_description, fine_tuned_generator):\n","    \"\"\"\n","    Generates a complete MRI diagnostic report that includes:\n","    1. MRI Findings\n","    2. Risk Assessment\n","    3. Treatment Recommendations\n","\n","    Parameters:\n","    - mri_description (str): The detailed description of MRI imaging observations.\n","    - fine_tuned_generator (Pipeline): The fine-tuned LLaMA model used for text generation.\n","\n","    Returns:\n","    - str: A formatted diagnostic report combining all sections.\n","    \"\"\"\n","\n","    # Step 1: Generate MRI Findings\n","    # ------------------------------\n","    # ✅ Generate MRI Findings section using the fine-tuned model\n","    raw_mri_findings = generate_mri_findings(mri_description, fine_tuned_generator)\n","    mri_findings = clean_mri_findings_output(raw_mri_findings)\n","\n","    # Step 2: Generate Risk Assessment\n","    # ---------------------------------\n","    # ✅ Generate the risk assessment using the MRI findings\n","    raw_risk_assessment = assess_mri_risk(mri_findings, fine_tuned_generator)\n","    risk_assessment = extract_output_section(raw_risk_assessment)\n","\n","    # Step 3: Generate Treatment Recommendations\n","    # -------------------------------------------\n","    # ✅ Generate the treatment plan using MRI findings and the risk assessment\n","    raw_treatment_plan = generate_treatment_plan(mri_findings, risk_assessment, fine_tuned_generator)\n","    treatment_plan = extract_output_section(raw_treatment_plan)\n","\n","    # Step 4: Combine All Sections into a Final Report\n","    # -------------------------------------------------\n","    # ✅ Format the content neatly with headers and spacing\n","    report = (\n","        \"================ MRI Diagnostic Report ================\\n\\n\"\n","        \"**MRI Findings:**\\n\"\n","        f\"{mri_findings}\\n\\n\"\n","        \"**Risk Assessment:**\\n\"\n","        f\"{risk_assessment}\\n\\n\"\n","        \"**Treatment Recommendations:**\\n\"\n","        f\"{treatment_plan}\\n\"\n","        \"========================================================\"\n","    )\n","\n","    # Step 5: Return the Final Report\n","    return report\n","\n","# Sample Usage\n","report = generate_mri_diagnostic_report(mri_description, fine_tuned_generator)\n","print(report)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6lDK5wlSl53I","executionInfo":{"status":"ok","timestamp":1742636193271,"user_tz":240,"elapsed":41,"user":{"displayName":"Gao Wayne","userId":"08103769368950608599"}},"outputId":"45ff096b-6fed-4afb-b5b6-93a4c616ca8c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["**MRI Findings:**\n","    Tumor Classification: Glioma (likely astrocytic or oligodendroglial in origin)\n","    Location: Right frontal lobe\n","    Size/Extent: 4.5 cm\n","    Type: Hyperintense on T2-weighted imaging, with irregular margins and heterogeneous\n","          enhancement following contrast administration\n","    Distribution Pattern: Intracranial mass with irregular margins, causing significant\n","                        midline shift and compression of the right lateral ventricle\n","    Mass Effect: Significant vasogenic edema present, causing midline shift of\n","                approximately 5 mm and partial effacement of the sulci \n","\n","**Risk Assessment:**\n","    Based on the MRI findings, the risk of malignancy is estimated to be [High (70/100)].\n","    The contribution to the risk score is as follows:\n","      - Tumor Classification: Contributes 20% to the total risk score. The glioma\n","        classification is consistent with a medium to high-grade tumor, which accounts for\n","        [80/100] of the total risk score.\n","      - Location: Contributes 15% to the total risk score. The right frontal lobe location\n","        is a medium-risk factor, accounting for [12/100] of the total risk score.\n","      - Size/Extent: Contributes 30% to the total risk score. The tumor size of 4.5 cm\n","        is a medium-risk factor, accounting for [45/100] of the total risk score.\n","      - Distribution Pattern: Contributes 15% to the total risk score. The irregular margins\n","        and significant midline shift with vasogenic edema are high-risk features, accounting for\n","        [22/100] of the total risk score.\n","      - Mass Effect: Contributes 20% to the total risk score. The significant mass effect\n","        with midline shift of approximately 5 mm and partial effacement of the sulci is a\n","        high-risk feature, accounting for [40/100] of the total risk score. \n","\n","**Treatment Recommendations:**\n","    Based on the MRI findings and risk assessment, the following treatment options are considered: \n","\n","- Surgical Resection: Surgical resection may be considered for this patient,\n","      given the high risk of malignancy and significant midline shift. The tumor's\n","      location in the right frontal lobe and its irregular margins may make it\n","      challenging to completely resect the tumor, but it is still the primary treatment option. \n","\n","- Radiation Therapy: Radiation therapy may be considered as an adjunct to\n","      surgical resection or as a standalone treatment for any remaining tumor after surgery.\n","      The patient's age and overall health status may affect the radiation therapy plan. \n","\n","- Conservative Management: Conservative management with close follow-up and\n","      serial MRI may be considered for this patient, given the high risk of malignancy and\n","      significant midline shift. This approach may be more appropriate for patients who are\n","      not candidates for surgical resection or radiation therapy. \n","\n","- Clinical Trials: The patient may be eligible for clinical trials of new treatments\n","      for glioma, given the high risk of malignancy and the patient's age and overall health\n","      status. \n","\n","Follow-up Schedule:\n","      MRI follow-up in 1 month to assess tumor stability and potential response to treatment. \n","\n"]}]}]}